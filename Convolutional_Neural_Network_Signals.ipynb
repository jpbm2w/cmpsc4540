{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpbm2w/cmpsc4540/blob/main/Convolutional_Neural_Network_Signals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional Neural Networks\n",
        "\n",
        "Convolutional Neural Networks (CNNs, ConvNets, etc.) can be a bit daunting at first glance. They're used extensively in machine learning and image processing today and they deal with mathematical concepts called convolutions. This notebook will show a quick implementation of a ConvNet with PyTorch 1.10.\n",
        "\n",
        "There are a couple things to pay attention to when designing a network:\n",
        "1. Make sure your Tensor shapes are correct\n",
        "2. Make sure your computer can handle the amount of weights. Each filter has it's own weights and it can explode\n",
        "3. Know if you want to include additional components such as batch normalization or Dropout (extended ideas). I've included BatchNorm here to combat tensors getting too compressed and causing learning issues inside the network\n",
        "4. Decide which activation function you want to use. We use a ReLU here\n",
        "5. Finally, decide how many output layers you're going to want. We design this with 1 single fully connected output layer\n",
        "\n",
        "Overall the structure is a sequential feed forward pattern through 6 convolutional blocks and then through the linear layer. If we wanted, we could add residuals, increase the number of blocks, change the block sizes, the parameters in each convolutional layer, etc. however it is meant to stay relatively simple.\n",
        "\n",
        "Note, this convolutional neural network is designed to output a binary classification for the problem \"Is there an embedded pure sine wave?\". If you want to change it to a regression problem or otherwise, you'll need to change the output shape of the network and the loss function (see the fit function)."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "JL4CgG92ch3F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "from typing import List\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels: int = 1,\n",
        "                 hidden_dims: List = None,\n",
        "                 output_length: int = 2) -> None:\n",
        "        super(ConvNet, self).__init__()\n",
        "\n",
        "        if hidden_dims is None:\n",
        "            hidden_dims = [32, 64, 128, 64, 32, 1]\n",
        "\n",
        "        modules = []\n",
        "\n",
        "        for h_dim in hidden_dims:\n",
        "            modules.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv1d(in_channels, out_channels=h_dim,\n",
        "                              kernel_size=3, stride=2, padding=1),\n",
        "                    nn.BatchNorm1d(h_dim),\n",
        "                    nn.ReLU())\n",
        "            )\n",
        "            in_channels = h_dim\n",
        "\n",
        "        self.forward_net = nn.Sequential(*modules)\n",
        "\n",
        "        # Assuming the input length is 5000, calculate the output length after the convolutions\n",
        "        conv_output_length = 5000  # Initialize with the input length\n",
        "        for _ in hidden_dims:\n",
        "            conv_output_length = (conv_output_length + 1) // 2  # This assumes stride=2, and padding=\"same\"\n",
        "\n",
        "        # Assuming the final element of hidden_dims is the output channel size of the last conv layer\n",
        "        final_conv_output_channels = hidden_dims[-1]\n",
        "        num_features_before_fc = conv_output_length * final_conv_output_channels\n",
        "\n",
        "        # Correct the input feature size of the first linear layer\n",
        "        self.fcfinal = nn.Linear(num_features_before_fc, output_length)\n",
        "        self.fcfinal = nn.Linear(num_features_before_fc, 1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_net(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor before passing to the fully connected layer\n",
        "        out = self.fcfinal(x)\n",
        "        return out"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "yoJ88qxDch3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Generation\n",
        "\n",
        "Here we start to generate data."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "5xOZ817kch3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yk4mCIDmw6pF",
        "outputId": "717ca6a8-ccf5-45b8-ce47-43e8c1b944a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "filepaths = [\n",
        "    '/content/drive/My Drive/Project3_LFP_data/Part1SubjectHB10.h5',\n",
        "    '/content/drive/My Drive/Project3_LFP_data/Part2SubjectHB13.h5',\n",
        "    '/content/drive/My Drive/Project3_LFP_data/Rat08-20130711_017.h5'\n",
        "]\n",
        "\n",
        "fs = [h5py.File(filename, 'r') for filename in filepaths]\n",
        "fss = [f.attrs['fs'][0] for f in fs]\n",
        "\n",
        "print(\"Sampling rates are:\\n\")\n",
        "for i in range(len(filepaths)):\n",
        "    print(str(filepaths[i]) + \": \" + str(fss[i]))\n",
        "\n",
        "sampling_rates = {\n",
        "    'Part1SubjectHB10.h5': fss[0],\n",
        "    'Part2SubjectHB13.h5': fss[1],\n",
        "    'Rat08-20130711_017.h5': fss[2],\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8PMZJpow4pN",
        "outputId": "2f52a415-0c37-447d-e7c9-d6b5bed706e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampling rates are:\n",
            "\n",
            "/content/drive/My Drive/Project3_LFP_data/Part1SubjectHB10.h5: 1000.0\n",
            "/content/drive/My Drive/Project3_LFP_data/Part2SubjectHB13.h5: 1000.0\n",
            "/content/drive/My Drive/Project3_LFP_data/Rat08-20130711_017.h5: 1250.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filestates = []\n",
        "for i in range(3):\n",
        "  states = []\n",
        "  print(\"File\" + str(filepaths[i]))\n",
        "  curFile = fs[i]\n",
        "  for name, grp in curFile.items():\n",
        "    states.append(name)\n",
        "    print(\"State: %s\" % (name))\n",
        "    print(\"Segment IDs:\")\n",
        "    print(list(grp.keys()))\n",
        "  filestates.append(states)\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQ5nY2xCxFCW",
        "outputId": "db065495-c2ec-402b-f9ee-2039fd718dfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File/content/drive/My Drive/Project3_LFP_data/Part1SubjectHB10.h5\n",
            "State: NREM\n",
            "Segment IDs:\n",
            "['1', '10', '11', '12', '13', '14', '15', '16', '17', '2', '3', '4', '5', '6', '7', '8', '9']\n",
            "State: WAKE\n",
            "Segment IDs:\n",
            "['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '34', '35', '36', '37', '38', '4', '5', '6', '7', '8', '9']\n",
            "\n",
            "\n",
            "File/content/drive/My Drive/Project3_LFP_data/Part2SubjectHB13.h5\n",
            "State: NREM\n",
            "Segment IDs:\n",
            "['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '3', '4', '5', '6', '7', '8', '9']\n",
            "State: WAKE\n",
            "Segment IDs:\n",
            "['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '3', '4', '5', '6', '7', '8', '9']\n",
            "\n",
            "\n",
            "File/content/drive/My Drive/Project3_LFP_data/Rat08-20130711_017.h5\n",
            "State: NREM\n",
            "Segment IDs:\n",
            "['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '4', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '5', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '6', '7', '8', '9']\n",
            "State: WAKE\n",
            "Segment IDs:\n",
            "['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '34', '35', '36', '37', '4', '5', '6', '7', '8', '9']\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lfps = []\n",
        "for j in range(3):\n",
        "  lfp = {key: [] for key in states}\n",
        "  for key in filestates[j]:\n",
        "    group = fs[j][key]\n",
        "    n = len(group)\n",
        "    for i in range(n):\n",
        "      lfp[key].append(group[str(i+1)][()].astype(float))\n",
        "  lfps.append(lfp)\n",
        "  print(lfp)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWbyOjQsxGA_",
        "outputId": "fa3e99e6-6f30-4866-8053-ee68e550dd91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'NREM': [array([-1129., -1152., -1170., ...,  2631.,  2593.,  2663.]), array([-492., -522., -298., ...,  -34.,  -81.,  -45.]), array([ -496.,  -431.,  -425., ..., -3005., -3175., -3350.]), array([ 395.,  353.,  349., ..., 1111.,  938.,  834.]), array([ 2233.,  2374.,  2504., ..., -1358., -1347., -1429.]), array([-930., -824., -797., ...,  508.,  419.,  231.]), array([1235., 1510., 1606., ...,  652.,  556.,  364.]), array([   79.,   256.,   267., ..., -2565., -2319., -2090.]), array([  -3., -122., -144., ..., -206., -327., -598.]), array([-1746., -1769., -1793., ...,   798.,   970.,  1058.]), array([ 726.,  692.,  542., ..., 2513., 2549., 2471.]), array([ -114.,  -138.,   -91., ..., -2226., -2608., -2942.]), array([ -216.,  -100.,   -58., ..., -2707., -3131., -3538.]), array([ 879.,  850.,  759., ..., 1246., 1362., 1409.]), array([ 148.,  167.,  159., ..., 1166., 1108., 1008.]), array([ -205.,  -243.,  -279., ..., -1241., -1018.,  -838.]), array([ 2907.,  2830.,  2655., ..., -1777., -1401., -1163.])], 'WAKE': [array([1192.,  983., 1254., ..., -285., -311., -547.]), array([ -293.,  -494.,  -617., ...,  -771., -1058., -1349.]), array([ 2977.,  2958.,  3032., ..., -1288., -1344., -1683.]), array([477., 593., 810., ..., 413., 569., 790.]), array([-1036., -1810., -2073., ...,  1504.,  1122.,   954.]), array([-1354., -1238., -1046., ..., -2641., -2068., -2058.]), array([ -314.,   167.,   472., ..., -1745., -1716., -1685.]), array([  591.,   453.,   340., ..., -2207., -1861., -1462.]), array([-1.000e+00, -6.100e+01, -6.100e+01, ..., -2.080e+03, -2.142e+03,\n",
            "       -2.243e+03]), array([  456.,   429.,   424., ..., -1065.,  -985.,  -949.]), array([-1260., -1449., -1640., ...,   685.,   549.,   256.]), array([ 735.,  810., 1033., ...,  620.,  508.,  364.]), array([1302., 1249., 1438., ..., -663., -500., -430.]), array([  365.,   523.,   901., ..., -1899., -1895., -1498.]), array([ 1018.,   800.,   461., ..., -3258., -2970., -2662.]), array([-427., -369., -247., ..., -408., -509., -583.]), array([1029., 1024., 1166., ..., 1367., 1418., 1404.]), array([ 923.,  898.,  780., ..., 1438., 1231.,  838.]), array([1117.,  620.,  210., ..., -281., -489., -696.]), array([ 793.,  999., 1055., ...,  342.,  836., 1238.]), array([ 1675.,  2287.,  2638., ..., -1888., -1698., -1416.]), array([  454.,   466.,    54., ..., -1146.,  -564.,  -180.]), array([ 908.,  757.,  740., ..., -779., -863., -880.]), array([-480., -583., -832., ..., 1514., 1527., 1161.]), array([ -903.,  -927.,  -867., ...,  -403., -1236., -1950.]), array([-76.,  -1., 143., ..., 542., 740., 845.]), array([ -540.,  -586.,  -708., ...,  -615.,  -923., -1476.]), array([-477., -264.,  189., ...,   37.,   59., -174.]), array([ -256.,  -430.,  -434., ..., -4659., -4560., -4584.]), array([-2015., -1947., -2106., ...,  -429.,  -620.,  -798.]), array([  607.,   525.,   655., ..., -3613., -3887., -4295.]), array([1192., 1250., 1141., ...,  -83., -798., -754.]), array([ -670.,  -648.,  -411., ..., -2320., -2234., -2169.]), array([-586., -644., -579., ..., -418., -567., -408.]), array([-1058.,  -651., -1031., ...,  1849.,  1916.,  1853.]), array([ -268.,  -411.,  -240., ...,  -826., -1425., -1767.]), array([ -309.,  -500.,  -535., ...,  -754.,  -959., -1052.]), array([-1926., -1854., -1693., ...,   -91.,  -259.,   -71.])]}\n",
            "{'NREM': [array([ -38.,   -4.,   45., ..., -348., -355., -312.]), array([-873., -859., -877., ..., 1338., 1242., 1210.]), array([-793., -771., -809., ...,  410.,  437.,  388.]), array([-143., -143., -106., ..., -257., -279., -250.]), array([ 872.,  846.,  867., ..., -373., -324., -378.]), array([ 292.,  190.,  147., ..., -885., -787., -687.]), array([ 87.,  27.,  15., ...,  13., -87., -83.]), array([149., 174.,  66., ..., 520., 553., 513.]), array([-1077., -1100.,  -963., ...,   337.,   247.,   300.]), array([ 77.,  77., 278., ..., 469., 424., 438.]), array([-192., -224., -291., ..., -743., -850., -876.]), array([1316., 1213., 1178., ...,   37.,   27.,  -68.]), array([  131.,   206.,   262., ..., -1021., -1178., -1245.]), array([-585., -691., -614., ...,  437.,  465.,  392.]), array([-333., -357., -543., ...,  155.,   71.,   85.]), array([-677., -688., -524., ...,  239.,  292.,  337.]), array([-298., -325., -380., ...,   73.,  153.,  133.]), array([ 617.,  517.,  624., ..., -685., -726., -802.]), array([-1063., -1033.,  -942., ...,  -399.,  -356.,  -337.])], 'WAKE': [array([-101., -244., -367., ...,  679.,  512.,  231.]), array([-686., -508., -713., ..., -156.,  223.,  397.]), array([ 574.,  712.,  509., ..., -126.,   56.,  -14.]), array([ 123.,  -39., -165., ...,  796.,  864., 1073.]), array([ 188.,  106.,  -34., ..., 1320., 1080., 1062.]), array([-243., -167., -145., ..., -160., -308., -437.]), array([-215., -234., -282., ...,  102.,   85., -128.]), array([353., 397., 456., ..., 467., 375., 159.]), array([  68.,   56.,  298., ..., -760., -770., -612.]), array([ -46.,  -68., -121., ...,  -35.,  142.,  208.]), array([-562., -566., -589., ..., -402., -332., -331.]), array([ 269.,  180., -116., ..., -378., -158., -142.]), array([1004., 1041., 1103., ...,  348.,  346.,  138.]), array([668., 620., 585., ..., 393., 379., 279.]), array([-749., -237.,  -22., ...,  503.,  399.,  389.]), array([-396., -364., -247., ...,  735.,  643.,  629.]), array([-253.,  -52.,  -63., ...,  268.,  173.,  118.]), array([ 171.,  145.,   14., ..., -658., -809., -823.]), array([-198., -213.,   22., ...,  376.,  315.,   38.]), array([587., 590., 592., ..., 106.,   1., 110.]), array([704., 447., 351., ..., 836., 951., 997.]), array([431., 443., 351., ..., 563., 595., 796.])]}\n",
            "{'NREM': [array([ -73.,  -52., -112., ..., -163.,  -80.,   32.]), array([ 563.,  765.,  689., ..., -270., -278., -271.]), array([1027., 1036., 1040., ...,  321.,  342.,  369.]), array([ 442.,  410.,  302., ..., -300., -331., -334.]), array([ -88., -118., -133., ..., -522., -491., -537.]), array([ 262.,  375.,  442., ..., -177., -179., -309.]), array([   76.,    60.,    17., ..., -4812., -4874., -4851.]), array([-563., -466., -455., ...,  600.,  571.,  575.]), array([-566., -579., -528., ..., 2401., 2311., 2215.]), array([-1180., -1225., -1269., ...,   213.,   259.,   312.]), array([ 932.,  985., 1005., ...,  253.,  317.,  304.]), array([ 418.,  398.,  421., ..., -133., -208., -115.]), array([-1168., -1153., -1116., ...,  -523.,  -539.,  -547.]), array([ 48.,  96., 101., ..., 370., 372., 376.]), array([-621., -589., -665., ...,  437.,  459.,  455.]), array([-781., -835., -903., ..., -514., -532., -560.]), array([ -54., -124., -138., ...,  405.,  369.,  384.]), array([-454., -400., -371., ...,  547.,  544.,  603.]), array([1004., 1043., 1055., ..., -268., -367., -295.]), array([-117.,  -64.,  -86., ..., 1176.,  863.,  707.]), array([  60.,   60.,   61., ..., -153., -235., -291.]), array([213., 155.,  60., ..., 349., 338., 321.]), array([ 524.,  559.,  632., ..., -162.,  -71.,  137.]), array([233., 306., 467., ...,  41.,  45.,  25.]), array([-479., -357., -235., ...,  520.,  555.,  546.]), array([-363., -374., -393., ...,  525.,  494.,  507.]), array([-304., -314., -336., ...,  636.,  575.,  584.]), array([-347., -222., -130., ...,  624.,  233.,  124.]), array([-553., -479., -448., ...,  944.,  906.,  906.]), array([ 460.,  479.,  476., ..., -413., -405., -438.]), array([1251., 1366., 1402., ...,  719.,  566.,  420.]), array([-534., -488., -471., ...,  -11.,   34.,   79.]), array([-242., -224., -208., ...,  151.,   78.,  110.]), array([275., 308., 273., ..., 209., 287., 303.]), array([-149., -271., -278., ...,  817.,  767.,  841.]), array([ 581.,  595.,  633., ..., -111., -123.,  -93.]), array([-226., -168., -153., ...,  925.,  937.,  983.]), array([-590., -565., -541., ...,  591.,  610.,  567.]), array([ -84.,  -94.,  -75., ..., -342., -363., -279.]), array([  6.,  16., -42., ..., 434., 444., 495.]), array([-756., -740., -665., ..., -295., -332., -384.]), array([-147.,  -67.,  -24., ..., -434., -407., -406.]), array([209.,  99.,  47., ..., 755., 623., 489.]), array([198., 223., 288., ..., 198., 199., 221.]), array([ -77., -110.,   38., ...,  357.,  565.,  578.]), array([ -77.,  -61.,  -49., ..., -486., -430., -392.]), array([-264., -327., -352., ...,   45.,   63.,   82.]), array([ -945., -1062., -1033., ...,   357.,   328.,   285.]), array([ 222.,  205.,  199., ..., -556., -582., -544.]), array([  10.,   59.,  167., ..., -214., -202., -223.]), array([-216., -338., -464., ..., 1111., 1185., 1216.]), array([ -71., -175., -203., ...,  169.,  147.,  154.]), array([-910., -961., -951., ...,  101.,  137.,  146.]), array([640., 641., 608., ...,  91., 191., 223.]), array([-681., -669., -627., ...,  867.,  882.,  852.]), array([-287., -304., -203., ..., -324., -167., -124.]), array([-134.,  -26.,   77., ..., -468., -440., -451.]), array([  94.,  106., -102., ..., -292., -268., -222.]), array([-5958., -6085., -6175., ...,   319.,   448.,   529.])], 'WAKE': [array([-5997., -5966., -5938., ...,   -21.,   -46.,   -64.]), array([ 517.,  568.,  541., ..., -351., -233., -104.]), array([-390., -374., -420., ...,  522.,  377.,  332.]), array([-647., -576., -371., ..., -438., -459., -454.]), array([ -75., -104., -108., ...,  426.,  416.,  472.]), array([482., 452., 385., ..., 305., 323., 338.]), array([ -998.,  -988., -1025., ...,   119.,   184.,   223.]), array([604., 626., 678., ..., 286., 309., 319.]), array([566., 628., 545., ...,  89.,  82.,   5.]), array([ 402.,  379.,  340., ..., -381., -433., -571.]), array([455., 409., 372., ..., 890., 653., 668.]), array([-18.,  53., 107., ...,  -5.,  10.,  -9.]), array([331., 307., 381., ..., 383., 395., 412.]), array([  97.,  132.,  166., ..., -541., -547., -522.]), array([ 716.,  811.,  756., ...,  963., 1090., 1085.]), array([-745., -745., -718., ..., -491., -469., -478.]), array([-38.,  61.,  51., ..., 238., 248., 219.]), array([ 183.,   65.,   -9., ..., -528., -495., -507.]), array([ 42., 142.,  94., ..., 373., 378., 387.]), array([-26.,  42.,  70., ..., 171.,  38.,  70.]), array([-202., -226., -101., ...,  816.,  804.,  763.]), array([ 887.,  892.,  830., ..., -394., -520., -545.]), array([ 69.,  90., 112., ..., 560., 519., 318.]), array([ 86.,  85., 128., ..., 301., 225., 226.]), array([ 477.,  490.,  530., ..., -707., -722., -738.]), array([-558., -564., -585., ...,  249.,  273.,  264.]), array([ -81., -240., -169., ..., -273., -279., -209.]), array([ 43.,  92., 175., ..., 318., 282., 294.]), array([373., 269., 159., ...,  -4.,  59., 115.]), array([389., 419., 320., ..., 236., 237., 241.]), array([-525., -519., -486., ...,   48.,   74.,   61.]), array([-226., -213., -154., ..., -378., -451., -591.]), array([-148., -171., -127., ..., 2419., 2412., 2422.]), array([-91.,   3.,  74., ..., 193.,  53.,  17.]), array([ -38.,  -78., -116., ...,    8.,  -25.,  -19.]), array([-215.,  -78., -151., ..., 1454., 1449., 1392.]), array([1355., 1257., 1209., ..., 1590., 1692., 1689.])]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clip_length_sec = 5\n",
        "subject_clips_labels = {}\n",
        "\n",
        "for j, filepath in enumerate(filepaths):\n",
        "    file_data = lfps[j]\n",
        "    sampling_rate = fss[j]\n",
        "    num_samples_per_clip = int(sampling_rate * clip_length_sec)\n",
        "\n",
        "    subject_clips = []\n",
        "    subject_labels = []\n",
        "\n",
        "    for state, segments in file_data.items():\n",
        "        label = 0 if state == 'NREM' else 1\n",
        "        for segment in segments:\n",
        "            num_clips = len(segment) // num_samples_per_clip\n",
        "            for i in range(num_clips):\n",
        "                start = i * num_samples_per_clip\n",
        "                end = start + num_samples_per_clip\n",
        "                subject_clips.append(segment[start:end])\n",
        "                subject_labels.append(label)\n",
        "\n",
        "    subject_clips = np.array(subject_clips)\n",
        "    subject_labels = np.array(subject_labels)\n",
        "\n",
        "    subject_identifier = filepath.split('/')[-1]\n",
        "    subject_clips_labels[subject_identifier] = (subject_clips, subject_labels)\n",
        "print(subject_clips)\n",
        "print(subject_labels)"
      ],
      "metadata": {
        "id": "qBpmGVKI4ZfJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e2ccddc-d7b3-40c3-c1fd-f1981acfed0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  -73.   -52.  -112. ...  1049.  1135.  1046.]\n",
            " [  996.   985.   963. ...   540.   539.   616.]\n",
            " [  642.   570.   588. ...  -117.   -77.  -101.]\n",
            " ...\n",
            " [  193.   148.   277. ...   974.  1051.  1019.]\n",
            " [  962.   900.   780. ...   710.   791.   774.]\n",
            " [  764.   859.   930. ... -4418. -4366. -4357.]]\n",
            "[0 0 0 ... 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "common_length = 5000\n",
        "all_segments = []\n",
        "all_labels = []\n",
        "\n",
        "for subject_identifier, (clips, labels) in subject_clips_labels.items():\n",
        "    for i, segment in enumerate(clips):\n",
        "        if len(segment) >= common_length:\n",
        "            all_segments.append(segment[:common_length])\n",
        "            all_labels.append(labels[i])\n",
        "\n",
        "data = torch.stack([torch.from_numpy(segment) for segment in all_segments], dim=0).unsqueeze(1)\n",
        "\n",
        "labels = torch.tensor(all_labels, dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "aw3yLrg2EVZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Combined Noise and Signal\n",
        "\n",
        "As can be seen in the graph below, the pure pink noise signal and the combined pink noise and sine signal is hardly distinguishable by the human eye. A CNN can make the distinction however."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "R01eHjRjch3N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'colorednoise'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-5d8d565e1b86>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcolorednoise\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# pink noise is 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'colorednoise'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import colorednoise as cn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "beta = 1  # pink noise is 1\n",
        "\n",
        "noise = cn.powerlaw_psd_gaussian(beta, samples)\n",
        "plt.plot(noise, label='Noise')\n",
        "plt.plot(noise+sine, label='Noise and Signal')\n",
        "plt.title('Colored Noise for Î²='+str(beta))\n",
        "plt.xlabel('Samples (time-steps)')\n",
        "plt.ylabel('Amplitude(t)', fontsize='large')\n",
        "plt.xlim(1,samples)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "EgwvQsTuch3N",
        "outputId": "3f2b34bf-0043-490e-dff8-191481888cc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Building the Dataset\n",
        "\n",
        "This is when we finally build the dataset. Note we specify the number of samples and let the code determine if the data contains a sine wave or not. This is done through a random choice so that we get a balanced dataset with roughly half the samples containing the sine wave and the other half as pure pink noise. Overall, the input data becomes 10000 samples of a single signal for 256 timesteps. The output is 10000 samples for 1 signal of 2 choices (Contains or Does not Contain a sine wave)."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "4_-Imkrsch3O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'cn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-0ecc57a76c28>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpowerlaw_psd_gaussian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cn' is not defined"
          ]
        }
      ],
      "source": [
        "data = []\n",
        "labels = []\n",
        "num_samples = 10000\n",
        "for i in range(num_samples):\n",
        "    if np.random.randint(low=0.0, high=2.0):\n",
        "        data.append(cn.powerlaw_psd_gaussian(beta, samples))\n",
        "        labels.append([1,0])\n",
        "    else:\n",
        "        data.append(cn.powerlaw_psd_gaussian(beta, samples) + sine)\n",
        "        labels.append([0,1])\n",
        "\n",
        "data = np.stack(data).reshape((num_samples, 1, -1))\n",
        "labels = np.stack(labels).reshape((num_samples, 1, -1))\n",
        "print(data.shape)\n",
        "print(labels.shape)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "npLuvppich3P",
        "outputId": "be0f5cd2-8b18-4638-8ed6-509ba0540a66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DataLoaders\n",
        "\n",
        "DataLoaders are a feature of PyTorch meant to make training easier. Below we just simply split the data to be a 75% training 25% validation split and pass them to the DataLoader with the batch size and whether or not to shuffle the data. Now we are ready to look at training."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "U_RRU8kbch3P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4126\n",
            "torch.Size([5502, 1, 5000])\n",
            "torch.Size([5502])\n",
            "Sample input type: torch.float32\n",
            "Sample label type: torch.float32\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "batch_size = 32\n",
        "split_idx = int(0.75 * data.shape[0])\n",
        "\n",
        "print(split_idx)\n",
        "print(data.shape)\n",
        "print(labels.shape)\n",
        "\n",
        "data = data.float()\n",
        "labels = labels.float()\n",
        "\n",
        "train_data = DataLoader(TensorDataset(data[:split_idx], labels[:split_idx]), batch_size=batch_size, shuffle=True)\n",
        "valid_data = DataLoader(TensorDataset(data[split_idx:], labels[split_idx:]), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "x_sample, y_sample = next(iter(train_data))\n",
        "print(\"Sample input type:\", x_sample.dtype)\n",
        "print(\"Sample label type:\", y_sample.dtype)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "sXScbFh4ch3Q",
        "outputId": "efe8f4a7-4884-4e5f-cc6a-0b819a2a342b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training\n",
        "\n",
        "This is an example of a training loop. Notice it takes both validation and training into account. While this is not necessary, we do it just so we can run both in parallel and maximize our efficiency.\n",
        "\n",
        "The key lines here are:\n",
        "```python\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "```\n",
        "and this is what allows for training. If you notice, these are only called when using a training dataset because these two lines are the actual backpropagation and updates of the weights in the network. If these lines are not called the network does not learn. The rest is basically built around trying to help the net learn easier (updatable learning rates, loss printing, tqdm, etc.)\n",
        "\n",
        "*If you want to change the loss function* take a look at the line that says:\n",
        "```python\n",
        "loss_func = nn.MSELoss() if 'loss_function' not in kwargs else kwargs.get('loss_function')\n",
        "```\n",
        "This basically means we can specify a loss function to the fit function call, or if you want simply change the function from Mean Squared Error here. Right"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "cw-ltkpDch3R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "from typing import Optional, Tuple\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "def fit(model: nn.Module,\n",
        "        training_loader: DataLoader,\n",
        "        validation_loader: DataLoader,\n",
        "        epochs: int = 50,\n",
        "        device: str = 'cpu',\n",
        "        write_losses: bool = False,\n",
        "        save_filepath: Optional[str] = None,\n",
        "        **kwargs) -> Tuple[List, List]:\n",
        "    \"\"\"\n",
        "    Function used to fit the specified model with the provided data\n",
        "    :param model: neural network pytorch model\n",
        "    :param training_loader: pytorch dataloader containing the training data\n",
        "    :param validation_loader: pytorch dataloader containing the validation data\n",
        "    :param epochs: number of epochs to train\n",
        "    :param device: which device to train the model on. Should be either \"cuda:0\" or \"cpu\"\n",
        "    :param write_losses: boolean flag as to report losses during training\n",
        "    :param save_filepath: path to save model, if not specified, no model is saved\n",
        "    :param kwargs: \"optim\" optimizer,\n",
        "                    \"loss_function\" loss function,\n",
        "                    \"decay_rate\" decay rate,\n",
        "                    \"model_save_path\" model save path,\n",
        "                    \"loss_save_path\" csv save path\n",
        "    :return: training and validation losses over each epoch\n",
        "    \"\"\"\n",
        "    train_loss_list = []\n",
        "    val_loss_list = []\n",
        "\n",
        "    # splitting the dataloaders to generalize code\n",
        "    data_loaders = {\"train\": training_loader, \"val\": validation_loader}\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005) if 'optim' not in kwargs else kwargs.get('optim')\n",
        "\n",
        "    loss_func = nn.MSELoss() if 'loss_function' not in kwargs else kwargs.get('loss_function')\n",
        "\n",
        "    decay_rate = .99995 if 'decay_rate' not in kwargs else kwargs.get('decay_rate')\n",
        "    lr_sch = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decay_rate)\n",
        "\n",
        "    temp_loss = 100000000000000.0\n",
        "\n",
        "    \"\"\"\n",
        "    You can easily adjust the number of epochs trained here by changing the number in the range\n",
        "    \"\"\"\n",
        "    for epoch in tqdm(range(epochs), position=0, leave=True):\n",
        "        train_loss = 0.0\n",
        "        val_loss = 0.0\n",
        "\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train(True)\n",
        "            else:\n",
        "                model.train(False)\n",
        "\n",
        "            running_loss = 0.0\n",
        "            for i, (x, y) in enumerate(data_loaders[phase]):\n",
        "                x = x.to(device)\n",
        "                output = model(x)\n",
        "                y = y.to(device)\n",
        "                loss = loss_func(torch.squeeze(output), torch.squeeze(y))\n",
        "\n",
        "                # backprop\n",
        "                optimizer.zero_grad()\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                # calculating total loss\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            if phase == 'train':\n",
        "                train_loss = running_loss\n",
        "                lr_sch.step()\n",
        "            else:\n",
        "                val_loss = running_loss\n",
        "\n",
        "        # shows total loss\n",
        "        if epoch % 10 == 0 and write_losses:\n",
        "            tqdm.write('{} train loss: {.6f} val loss: {.6f}'.format(epoch + 1, train_loss, val_loss))\n",
        "\n",
        "        # saving best model\n",
        "        if train_loss < temp_loss and save_filepath:\n",
        "            torch.save(model, save_filepath)\n",
        "            temp_loss = train_loss\n",
        "\n",
        "        train_loss_list.append(train_loss)\n",
        "        val_loss_list.append(val_loss)\n",
        "\n",
        "\n",
        "    return train_loss_list, val_loss_list"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "OxgGuzQ5ch3S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Running the Model\n",
        "\n",
        "Here its pretty straightforward, simply pass the model, dataloaders, number of epochs, and device to the fit function. We also specify a loss function just to show how it can be done."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "zwzC_h5Hch3T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87560d5e995046ffb44774fa17ba9e25"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import copy\n",
        "\n",
        "epochs = 10\n",
        "model = ConvNet()  # Create an instance of your updated ConvNet class\n",
        "model_initial = copy.deepcopy(model)  # Deep copy isn't strictly necessary unless you're planning on comparing the initial and final model\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)  # Move the model to the GPU if available\n",
        "\n",
        "# Now you can call the fit function to train your model\n",
        "t_loss, v_loss = fit(model, train_data, valid_data, epochs, device, loss_function=nn.BCEWithLogitsLoss())\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "W71CEMs_ch3T",
        "outputId": "2c9a1c93-c64d-4054-90d0-13806e462b5b",
        "colab": {
          "referenced_widgets": [
            "87560d5e995046ffb44774fa17ba9e25",
            "54f2646779064834be9b935222e5a223",
            "2626cfd645d4431a82d3ac1ef8ccf754",
            "8cb456ad37cf4dfa8c0f53a4311cc987",
            "bd8b97d392f749338e68afa5e09914e7",
            "bdf15c8e125a4272b805eccea916814c",
            "b7e1a2fec5484bd8843c89264ea8ab47",
            "2a97e9aed3c64e0f90dea9b04ac84e74",
            "64059bf2d5ab4d7d991a4dd6a1357c45",
            "7331746bd40e4005a4fce730d90c90f1",
            "25537d1851e846e49cf8b80a4418e90d"
          ],
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Validation\n",
        "\n",
        "Here we just calculate the results of the validation training dataset to show it in a confusion matrix. Since we are calculating a Binary True/False problem a confusion matrix works perfectly to show how well the model is working."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "3EFnrWiych3U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1376,) (1376,)\n"
          ]
        }
      ],
      "source": [
        "def validate_model(model: nn.Module,\n",
        "                   validation_loader: torch.utils.data.DataLoader,\n",
        "                   device: torch.device) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    model.eval()  # Ensure the model is in evaluation mode\n",
        "    output_list = []\n",
        "    y_list = []\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        for i, (x, y) in enumerate(validation_loader):\n",
        "            x = x.to(device)\n",
        "            output = model(x)\n",
        "            output_list.append(output.to(\"cpu\").detach().numpy())\n",
        "            y_list.append(y.to(\"cpu\").detach().numpy())\n",
        "\n",
        "    # Concatenate all batches\n",
        "    output_list = np.concatenate(output_list, axis=0)\n",
        "    y_list = np.concatenate(y_list, axis=0)\n",
        "\n",
        "    # Apply sigmoid and threshold to convert logits to class predictions\n",
        "    pred_classes = (torch.sigmoid(torch.from_numpy(output_list)) > 0.5).numpy().astype(np.int32)\n",
        "\n",
        "    return np.squeeze(pred_classes), np.squeeze(y_list)\n",
        "\n",
        "preds, labels = validate_model(model, valid_data, device)\n",
        "print(preds.shape, labels.shape)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "bUq1HiBsch3V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91601759-928c-4ff5-af11-b82afae56b94"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGwCAYAAAB7MGXBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA/EUlEQVR4nO3de1xUdf7H8fcAcpGrmjKSeClXlLIs3TW6iBQrFr/U8ldaaHjJtsLK3DLd8pKWpNtVs6xWRfenpV100y1bM1BLxEvRWhmlaWKKVigIynXO7w9zagInxpnj4PR67uM8HnHOd77ne9gB33y+33PGYhiGIQAAAC/x8/YAAADA7xthBAAAeBVhBAAAeBVhBAAAeBVhBAAAeBVhBAAAeBVhBAAAeFWAtwdwNrPZbNq/f7/Cw8NlsVi8PRwAgIsMw9DRo0cVExMjPz9z/j6vqKhQVVWVR/oKDAxUcHCwR/pqTAgjbti/f79iY2O9PQwAgJsKCwvVpk0bj/dbUVGhDu0iVHSo2iP9Wa1W7d692+cCCWHEDeHh4Sf+o22y5Me3EgDOOrYaae/7P/8+97CqqioVHarW3o8vUUS4v1t9lR6tVdtLP1FVVRVhBD+zT834BUh+Tbw7GADAaTN7qj0i1E8RoW5OA9l899NbCCMAAJjNME5s7vbhowgjAACYjTDiFLf2AgAAr6IyAgCAySiMOEcYAQDAZIbhJ8NwbzLC8OE0wjQNAADwKiojAACYzDAsHqiM2Dw0msaHMAIAgMlshp9sboYRd1/fmPnulQEAgLMClREAAEzmmQWsvls/IIwAAGAywohzvntlAADgrEBlBAAAk524m8a9D+Nz9/WNGWEEAACTMU3jHGEEAACT2QyLbG5WNtx9fWPmuzELAACcFaiMAABgMkMemKbx4foBYQQAAJOxgNU5341ZAADgrEBlBAAAk1EZcY4wAgCAybi11znfvTIAAHBWoDICAIDJDMP9aRbD8NBgGiHCCAAAJmPNiHNM0wAAAK+iMgIAgMkMDzwO3pcrI4QRAABMxjSNc4QRAABMZsjP7ce5+/Lj4H33ygAAwFmByggAACZjmsY5wggAACYjjDjHNA0AAPAqKiMAAJiMyohzhBEAAExGGHGOaRoAAOBVVEYAADAZlRHnCCMAAJjM5oHHwbv7+saMaRoAAOBVVEYAADAZ0zTOURkBAMBkJ8OIu5sr1q9fr+uvv14xMTGyWCxasWLFr8ZkaNKkSWrdurVCQkKUnJysr7/+2qFNcXGx0tLSFBERoaioKI0cOVJlZWUObf773//qqquuUnBwsGJjYzVz5kyXvz+EEQAATOaNMFJeXq6LL75Yc+bMqff4zJkzNWvWLM2dO1d5eXkKDQ1VSkqKKioq7G3S0tL0+eefa82aNVq1apXWr1+vO+64w368tLRUffr0Ubt27bRt2zb9/e9/15QpU/Tyyy+7NFamaQAA8EHXXnutrr322nqPGYahZ599Vo888oj69+8vSVq0aJGio6O1YsUKDR48WDt27NDq1au1ZcsW9ejRQ5I0e/ZsXXfddXryyScVExOjxYsXq6qqSvPnz1dgYKAuuOAC5efn6+mnn3YILb+FyggAACbzZGWktLTUYausrHR5PLt371ZRUZGSk5Pt+yIjI9WzZ0/l5uZKknJzcxUVFWUPIpKUnJwsPz8/5eXl2dv06tVLgYGB9jYpKSkqKCjQ4cOHGzwewggAACYzJBmGm9tPfcXGxioyMtK+ZWZmujyeoqIiSVJ0dLTD/ujoaPuxoqIitWrVyuF4QECAmjdv7tCmvj5+eY6GYJoGAICzSGFhoSIiIuxfBwUFeXE0nkFlBAAAkxmyeGSTpIiICIftdMKI1WqVJB08eNBh/8GDB+3HrFarDh065HC8pqZGxcXFDm3q6+OX52gIwggAACbzxt00znTo0EFWq1Vr16617ystLVVeXp4SEhIkSQkJCTpy5Ii2bdtmb/PBBx/IZrOpZ8+e9jbr169XdXW1vc2aNWsUFxenZs2aNXg8hBEAAHxQWVmZ8vPzlZ+fL+nEotX8/Hzt3btXFotFY8aM0WOPPaa3335b27dv12233aaYmBgNGDBAktSlSxf17dtXo0aN0ubNm/XRRx9p9OjRGjx4sGJiYiRJt956qwIDAzVy5Eh9/vnnWrp0qZ577jmNHTvWpbGyZgQAALN5orLh4uu3bt2qpKQk+9cnA0J6erqysrI0btw4lZeX64477tCRI0d05ZVXavXq1QoODra/ZvHixRo9erSuueYa+fn5aeDAgZo1a5b9eGRkpP7zn/8oIyND3bt31znnnKNJkya5dFuvJFkMwzB+uxnqU1paqsjISKl9X8mvibeHAwBwla1a2rNaJSUlDotCPeXkvxPb/nOzwkIDf/sFTpSVV6l7n2WmjdWbmKYBAABexTQNAAAmsxknNnf78FWEEQAATMan9jpHGAEAwGSEEedYMwIAALyKyggAACajMuIcYQQAAJOd/LA7d/vwVUzTAAAAr6IyAgCAyX75QXfu9OGrCCMAAJiMNSPOMU0DAAC8isoIAAAmozLiHGEEAACTcTeNc0zTAAAAr6IyAgCAyZimcY4wAgCAyZimcY4wAgCAyaiMOMeaEQAA4FVURgAAMBmVEecIIwAAmMz20+ZuH76KaRoAAOBVVEYAADCbB6ZpxDQNAAA4XawZcY5pGgAA4FVURgAAMJkhDzz0zCMjaZwIIwAAmIxpGueYpgEAAF5FZQQAAJPx2TTOEUYAADAZ0zTOEUYAADCZIfcXoPpwYYQ1IwAAwLuojAAAYDKmaZwjjAAAYDIWsDrHNA0AAPAqwgjOGnffeFzfvPmjjmV/r9xXDuuPXaq9PSTAo3iP+66T0zTubr6KMIKzws3XVOipe8s0dX6oug9vpv/uDNDqZ0rUspnN20MDPIL3uG87OU3j7uarvBpGhg0bJovFoieeeMJh/4oVK2SxnEiAOTk5slgs9q1ly5a67rrrtH379nr7+vXWt29fe5v27dvLYrHotddeqzOWCy64QBaLRVlZWZ6/ULjt/sHH9Y+3g5X172Dt2BOgO2eG6VilRSP+p8LbQwM8gvc4fs+8XhkJDg7WjBkzdPjwYaftCgoKdODAAb333nuqrKxUamqqqqqqHNr07dtXBw4ccNheffVVhzaxsbFasGCBw75NmzapqKhIoaGhnrkoeFSTAEPd42r0/tZA+z7DsOj9LU102YWUsXH24z3u+5imcc7rYSQ5OVlWq1WZmZlO27Vq1UpWq1WXXnqpxowZo8LCQn355ZcObYKCgmS1Wh22Zs2aObRJS0vTunXrVFhYaN83f/58paWlKSCAm4sao3OibAoIkA4WO75dDxX7ydqcEjbOfrzHfZ/hoc1XeT2M+Pv7a/r06Zo9e7b27dv3m+1LSkrs0yyBgYG/0bqu6OhopaSkaOHChZKkY8eOaenSpRoxYsRvvrayslKlpaUOGwAAcI/Xw4gk3XDDDerWrZsmT558yjZt2rRRWFiYoqKitGTJEvXr10+dO3d2aLNq1SqFhYU5bNOnT6/T14gRI5SVlSXDMPTGG2/o/PPPV7du3X5znJmZmYqMjLRvsbGxLl8rXPfDET/V1EjRv/oLsVVzm4qKG8VbGHAL73HfxzSNc43mXT5jxgwtXLhQO3bsqPf4hg0btG3bNmVlZalTp06aO3dunTZJSUnKz8932O6888467VJTU1VWVqb169dr/vz5DaqKSNKECRNUUlJi33451QPzVNdYtK0gQNd0/3mNkMVi6Joe1dr0WRMvjgzwDN7jvo+7aZxrNIskevXqpZSUFE2YMEHDhg2rc7xDhw6KiopSXFycDh06pEGDBmn9+vUObUJDQ9WxY8ffPFdAQICGDh2qyZMnKy8vT8uXL2/QGIOCghQUFNSgtvCsZ14LUdYjR7X1yyba/EWAxgw6rtBgQwtWBXt7aIBH8B73bTyB1blGE0Yk6YknnlC3bt0UFxfntF1GRoYyMzO1fPly3XDDDad1rhEjRujJJ5/UoEGD6ixyReOzbG2wWkYZenRUuazNbcr/OkDXjo3UocONprgHuIX3OH7PGlUY6dq1q9LS0jRr1iyn7Zo2bapRo0Zp8uTJGjBggP2ZJJWVlSoqKnJoGxAQoHPOOadOH126dNEPP/ygpk2beu4CYKo5b4Zozpsh3h4GYBre477rRGXE3Q/K89BgGqFGF7mnTp0qm+23b2UbPXq0duzYoddff92+b/Xq1WrdurXDduWVV56yjxYtWigkhB98AIC5WDPinMUwfPnyzFVaWqrIyEipfV/Jj0VmAHDWsVVLe1arpKREERERHu/+5L8Tbyz4i0KburfmsPxYpf53+EumjdWbGtU0DQAAvskiQ+7emuu7t/YSRgAAMBl30zjX6NaMAACA3xcqIwAAmIzKiHOEEQAATOaJx7nzOHgAAACTUBkBAMBkNuPE5m4fvorKCAAAJjN+urXX3c0VtbW1mjhxojp06KCQkBCdf/75mjZtmn75eDHDMDRp0iS1bt1aISEhSk5O1tdff+3QT3FxsdLS0hQREaGoqCiNHDlSZWVlHvm+nEQYAQDAZN54AuuMGTP04osv6vnnn9eOHTs0Y8YMzZw5U7Nnz7a3mTlzpmbNmqW5c+cqLy9PoaGhSklJUUVFhb1NWlqaPv/8c61Zs0arVq3S+vXrdccdd3jqWyOJaRoAAHzSxo0b1b9/f6WmpkqS2rdvr1dffVWbN2+WdKIq8uyzz+qRRx5R//79JUmLFi1SdHS0VqxYocGDB2vHjh1avXq1tmzZoh49ekiSZs+ereuuu05PPvmkYmJiPDJWKiMAAJjMkAcqIz/1VVpa6rBVVlbWe87LL79ca9eu1VdffSVJ+vTTT/Xhhx/q2muvlSTt3r1bRUVFSk5Otr8mMjJSPXv2VG5uriQpNzdXUVFR9iAiScnJyfLz81NeXp7Hvj9URgAAMJknb+2NjY112D958mRNmTKlTvvx48ertLRUnTt3lr+/v2pra/X4448rLS1Nkuyfch8dHe3wuujoaPuxoqIitWrVyuF4QECAmjdvbm/jCYQRAADOIoWFhQ4flBcUVP8H8C1btkyLFy/WkiVLdMEFFyg/P19jxoxRTEyM0tPTz9RwG4QwAgCAyTz5BNaIiIgGfWrvgw8+qPHjx2vw4MGSpK5du+rbb79VZmam0tPTZbVaJUkHDx5U69at7a87ePCgunXrJkmyWq06dOiQQ781NTUqLi62v94TWDMCAIDJvHE3zbFjx+Tn5/jPvL+/v2w2mySpQ4cOslqtWrt2rf14aWmp8vLylJCQIElKSEjQkSNHtG3bNnubDz74QDabTT179jzN70ZdVEYAAPBB119/vR5//HG1bdtWF1xwgT755BM9/fTTGjFihCTJYrFozJgxeuyxx/SHP/xBHTp00MSJExUTE6MBAwZIkrp06aK+fftq1KhRmjt3rqqrqzV69GgNHjzYY3fSSIQRAABMdzoPLauvD1fMnj1bEydO1N13361Dhw4pJiZGf/nLXzRp0iR7m3Hjxqm8vFx33HGHjhw5oiuvvFKrV69WcHCwvc3ixYs1evRoXXPNNfLz89PAgQM1a9Yst67l1yyG4e4s1u9XaWmpIiMjpfZ9Jb8m3h4OAMBVtmppz2qVlJQ0aB2Gq07+O7HghdFqGlL/QtOGOna8UsPvft60sXoTa0YAAIBXMU0DAIDJPHk3jS8ijAAAYDLCiHOEEQAATObJJ7D6ItaMAAAAr6IyAgCAyQz9/EF37vThqwgjAACYjDUjzjFNAwAAvIrKCAAAZvNAZcSX52kIIwAAmIy7aZxjmgYAAHgVlREAAEzG3TTOEUYAADAZd9M4xzQNAADwKiojAACYjMqIc4QRAABMxt00zhFGAAAwGZUR51gzAgAAvIrKCAAAJuPWXucIIwAAmIxpGueYpgEAAF5FZQQAAJNRGXGOMAIAgMm4tdc5pmkAAIBXURkBAMBk3E3jHGEEAACTGfLAmhGPjKRxIowAAGAyFrA6x5oRAADgVVRGAAAwGZUR5wgjAACYjDDiHNM0AADAq6iMAABgMkMWGXLzoWduvr4xI4wAAGAypmmcY5oGAAB4FZURAADMxiNYnSKMAABgNg9M0/zuw8jbb7/d4A779et32oMBAAC/Pw0KIwMGDGhQZxaLRbW1te6MBwAAn8MsjXMNCiM2m83scQAA4LO4m8Y5t9aMVFRUKDg42FNjAQDAJxFGnHP51t7a2lpNmzZN5557rsLCwvTNN99IkiZOnKh58+Z5fIAAAMC3uRxGHn/8cWVlZWnmzJkKDAy077/wwgv1j3/8w6ODAwDAF5ysjLi7+SqXw8iiRYv08ssvKy0tTf7+/vb9F198sb788kuPDg4AAF9geGjzVS6Hke+++04dO3ass99ms6m6utojgwIAAL8fLoeR+Ph4bdiwoc7+N954Q5dccolHBgUAgC9hmsY5l++mmTRpktLT0/Xdd9/JZrPprbfeUkFBgRYtWqRVq1aZMUYAAM5q3E3jnMuVkf79+2vlypV6//33FRoaqkmTJmnHjh1auXKl/vznP5sxRgAA4MNO6zkjV111ldasWePpsQAA4JOojDh32g8927p1q3bs2CHpxDqS7t27e2xQAAD4Eh4H75zLYWTfvn265ZZb9NFHHykqKkqSdOTIEV1++eV67bXX1KZNG0+PEQAA+DCX14zcfvvtqq6u1o4dO1RcXKzi4mLt2LFDNptNt99+uxljBADgrMbdNM65XBlZt26dNm7cqLi4OPu+uLg4zZ49W1dddZVHBwcAgC8wDIsMw+J2H77K5TASGxtb78PNamtrFRMT45FBAQDgS1jA6pzL0zR///vfdc8992jr1q32fVu3btV9992nJ5980qODAwAAvq9BYaRZs2Zq3ry5mjdvruHDhys/P189e/ZUUFCQgoKC1LNnT3388ccaMWKE2eMFAOCs463Ppvnuu+80ZMgQtWjRQiEhIeratatDMcEwDE2aNEmtW7dWSEiIkpOT9fXXXzv0UVxcrLS0NEVERCgqKkojR45UWVnZaYzm1Bo0TfPss8969KQAAPyeeGOa5vDhw7riiiuUlJSkd999Vy1bttTXX3+tZs2a2dvMnDlTs2bN0sKFC9WhQwdNnDhRKSkp+uKLLxQcHCxJSktL04EDB7RmzRpVV1dr+PDhuuOOO7RkyRL3LugXGhRG0tPTPXZCAABw+kpLSx2+PjlL8WszZsxQbGysFixYYN/XoUMH+38bhqFnn31WjzzyiPr37y9JWrRokaKjo7VixQoNHjxYO3bs0OrVq7Vlyxb16NFDkjR79mxdd911evLJJz22VtTlNSO/VFFRodLSUocNAAA48uStvbGxsYqMjLRvmZmZ9Z7z7bffVo8ePXTTTTepVatWuuSSS/TKK6/Yj+/evVtFRUVKTk6274uMjFTPnj2Vm5srScrNzVVUVJQ9iEhScnKy/Pz8lJeX57Hvj8t305SXl+uhhx7SsmXL9OOPP9Y5Xltb65GBAQDgKwwZMtycpzF+WjVSWFioiIgI+/76qiKS9M033+jFF1/U2LFj9be//U1btmzRvffeq8DAQKWnp6uoqEiSFB0d7fC66Oho+7GioiK1atXK4XhAQICaN29ub+MJLoeRcePGKTs7Wy+++KKGDh2qOXPm6LvvvtNLL72kJ554wmMDAwAAdUVERDiEkVOx2Wzq0aOHpk+fLkm65JJL9Nlnn2nu3LmNbvmFy9M0K1eu1AsvvKCBAwcqICBAV111lR555BFNnz5dixcvNmOMAACc1bxxN03r1q0VHx/vsK9Lly7au3evJMlqtUqSDh486NDm4MGD9mNWq1WHDh1yOF5TU6Pi4mJ7G09wOYwUFxfrvPPOk3QinRUXF0uSrrzySq1fv95jAwMAwGd4Yr2Ii2nkiiuuUEFBgcO+r776Su3atZN0YjGr1WrV2rVr7cdLS0uVl5enhIQESVJCQoKOHDmibdu22dt88MEHstls6tmz5+l9L+rhchg577zztHv3bklS586dtWzZMkknKiYnPzgPAAB41/33369NmzZp+vTp2rlzp5YsWaKXX35ZGRkZkiSLxaIxY8boscce09tvv63t27frtttuU0xMjAYMGCDpRCWlb9++GjVqlDZv3qyPPvpIo0eP1uDBgz361HWX14wMHz5cn376qRITEzV+/Hhdf/31ev7551VdXa2nn37aYwMDAMBXeOM5I3/84x+1fPlyTZgwQVOnTlWHDh307LPPKi0tzd5m3LhxKi8v1x133KEjR47oyiuv1OrVq+3PGJGkxYsXa/To0brmmmvk5+engQMHatasWe5dzK9YDDeX93777bfatm2bOnbsqIsuushT4zorlJaWKjIyUmrfV/Jr4u3hAABcZauW9qxWSUlJgxaFuurkvxMPPXj/Ke96aajKykrN+Pszpo3Vm1yujPxau3bt7PNPAACgrtN9nPuv+/BVDQojrpRj7r333tMeDAAA+P1pUBh55plnGtSZxWL5XYaR7ue1lX9AoLeHAZgid+EWbw8BME3p0Ro162T+ebyxZuRs0qAwcvLuGQAA4DrCiHNufTYNAACAu9xewAoAAJxjAatzhBEAAExmGB74oDwfnqdhmgYAAHgVlREAAEzGAlbnTqsysmHDBg0ZMkQJCQn67rvvJEn//Oc/9eGHH3p0cAAA+AJ3PyTPE2GmMXM5jLz55ptKSUlRSEiIPvnkE1VWVkqSSkpKNH36dI8PEAAA+DaXw8hjjz2muXPn6pVXXlGTJj9/HssVV1yhjz/+2KODAwDANxge2nyTy2tGCgoK1KtXrzr7IyMjdeTIEU+MCQAAn8KaEedcroxYrVbt3Lmzzv4PP/xQ5513nkcGBQCALzl5a6+7m69yOYyMGjVK9913n/Ly8mSxWLR//34tXrxYDzzwgO666y4zxggAAHyYy9M048ePl81m0zXXXKNjx46pV69eCgoK0gMPPKB77rnHjDECAHBWY5rGOZfDiMVi0cMPP6wHH3xQO3fuVFlZmeLj4xUWFmbG+AAAOOvxOHjnTvuhZ4GBgYqPj/fkWAAAwO+Qy2EkKSlJFovllMc/+OADtwYEAICv4bNpnHM5jHTr1s3h6+rqauXn5+uzzz5Tenq6p8YFAIDvYJ7GKZfDyDPPPFPv/ilTpqisrMztAQEAgN8Xj31q75AhQzR//nxPdQcAgM/g+avOeexTe3NzcxUcHOyp7gAA8BmsGXHO5TBy4403OnxtGIYOHDigrVu3auLEiR4bGAAA+H1wOYxERkY6fO3n56e4uDhNnTpVffr08djAAADwFTz0zDmXwkhtba2GDx+url27qlmzZmaNCQAAn0IYcc6lBaz+/v7q06cPn84LAIBLDLf/58tLWF2+m+bCCy/UN998Y8ZYAADA75DLYeSxxx7TAw88oFWrVunAgQMqLS112AAAgKOT0zTubr6qwWtGpk6dqr/+9a+67rrrJEn9+vVzeCy8YRiyWCyqra31/CgBADjb+XCYcFeDw8ijjz6qO++8U9nZ2WaOBwAA/M40OIycfNhKYmKiaYMBAMAX8dE0zrl0a6+zT+sFAAD14wmszrkURjp16vSbgaS4uNitAQEAgN8Xl8LIo48+WucJrAAAwDkeeuacS2Fk8ODBatWqlVljAQDAJxFGnGvwc0ZYLwIAAMzg8t00AADANdxN41yDw4jNZjNzHAAA+CzupnHOpTUjAADAdawZcc7lz6YBAADwJMIIAADwKqZpAAAwGdM0zlEZAQAAXkVlBAAAk1EZcY4wAgCAybi11zmmaQAAgFdRGQEAwGQ8gdU5wggAACZjzYhzTNMAAACvojICAIDJqIw4RxgBAMBkrBlxjjACAIDZKI04xZoRAADgVYQRAABMdrIw4u52up544glZLBaNGTPGvq+iokIZGRlq0aKFwsLCNHDgQB08eNDhdXv37lVqaqqaNm2qVq1a6cEHH1RNTc3pD+QUCCMAAJjM8NB2OrZs2aKXXnpJF110kcP++++/XytXrtTrr7+udevWaf/+/brxxhvtx2tra5Wamqqqqipt3LhRCxcuVFZWliZNmnSaIzk1wggAAGeR0tJSh62ysvKUbcvKypSWlqZXXnlFzZo1s+8vKSnRvHnz9PTTT+vqq69W9+7dtWDBAm3cuFGbNm2SJP3nP//RF198of/7v/9Tt27ddO2112ratGmaM2eOqqqqPHpNhBEAAMzmiSman0ojsbGxioyMtG+ZmZmnPG1GRoZSU1OVnJzssH/btm2qrq522N+5c2e1bdtWubm5kqTc3Fx17dpV0dHR9jYpKSkqLS3V559/7rnvjbibBgAA03nyZprCwkJFRETY9wcFBdXb/rXXXtPHH3+sLVu21DlWVFSkwMBARUVFOeyPjo5WUVGRvc0vg8jJ4yePeRJhBACAs0hERIRDGKlPYWGh7rvvPq1Zs0bBwcFnaGSnj2kaAABMdqbvptm2bZsOHTqkSy+9VAEBAQoICNC6des0a9YsBQQEKDo6WlVVVTpy5IjD6w4ePCir1SpJslqtde6uOfn1yTaeQhgBAMBkJ5Z8uPu/hrvmmmu0fft25efn27cePXooLS3N/t9NmjTR2rVr7a8pKCjQ3r17lZCQIElKSEjQ9u3bdejQIXubNWvWKCIiQvHx8R76zpzANA0AAD4mPDxcF154ocO+0NBQtWjRwr5/5MiRGjt2rJo3b66IiAjdc889SkhI0GWXXSZJ6tOnj+Lj4zV06FDNnDlTRUVFeuSRR5SRkXHKdSqnizACAIDJGuPT4J955hn5+flp4MCBqqysVEpKil544QX7cX9/f61atUp33XWXEhISFBoaqvT0dE2dOtWzAxFhBAAA0zWGMJKTk+PwdXBwsObMmaM5c+ac8jXt2rXTO++8496JG4AwAgCAyfjUXudYwAoAALyKyggAAGeCL5c23EQYAQDAZI1hzUhjxjQNAADwKiojAACYjAWszhFGAAAwGdM0zjFNAwAAvIrKCAAAJqMy4hxhBAAAkxmGIcPNNOHu6xszpmkAAIBXURkBAMBk3E3jHGEEAACTsWbEOcIIAAAmI4w4x5oRAADgVVRGAAAwGWtGnCOMAABgMqZpnGOaBgAAeBWVEQAATEZlxDnCCAAAJmPNiHNM0wAAAK+iMgIAgNk8ME3jy6URwggAACZjzYhzTNMAAACvojICr7rtfw6od4/Date6QpXVftr+dZjmLG2jvUXB9jYPDdujP15wVOc0q9LxCn9t3xmmOUvP1bcHQuxtNi3aWqfvR+acp/fzmp+R6wBOWp/fRE8uCdXHXwbowI/+ejPziAb0qrQffysnSC+tCNHHBU1UXOqnbQt+VLdONfX2ZRhS6gNRem9TUJ1+7nsmXBu3N9Fn3wSoS7safbyw2PRrw+ljAatzhBF41SWdj+rN91vpi92h8vczdNdN3+m5cV/plvEXqKLKX5L05Z5QvZfbQgd/DFREaI1uv2G/nhv3tW4c21U2w2Lva9rL7ZW7PdL+ddkx/zN+PUD5cYsu7lit4anH9b9/i6p7vMKiKy+q1k1XV+ovMyKc9vXc0qayODk+PPW48r5oou07+VXe2DFN45xXp2nmzp2r8PBw1dT8/FdBWVmZmjRpot69ezu0zcnJkcVi0a5duyRJubm58vf3V2pqap1+9+zZI4vFovz8fPu+o0ePKikpSfHx8dq3b5+9TX3bpk2bTLle1HX/k5307w/P0e7vQrSzsKmmvdJerc+pUucOx+xt/pXTUvkF4TrwQ5AKvg3VS2+eK2uLKrVuWenQ19Fj/iouaWLfqqqZhcSZd21ClabdUa4bEivrPT60b4UmjihX8h/rP35S/lcBevq1ppr3t9J6jz93/1HdPfC4zoupdXvMMN/JMOLu5qu8+ts6KSlJZWVl2rr15xL7hg0bZLValZeXp4qKCvv+7OxstW3bVueff74kad68ebrnnnu0fv167d+/3+l5vv/+eyUlJam8vFwbNmxQmzZt7Mfef/99HThwwGHr3r27h68UDRUWcuIXa2lZ/X/pBQfWKvWqH/TdoUAd/DHQ4dgDt+3V6jn5mjf5C/1Prx/k20VN+LJjFdKQRyM1+69HZW1h8/ZwANN5tbYXFxen1q1bKycnR5dddpmkExWQ/v3764MPPtCmTZvsFZKcnBwlJSVJOlE9Wbp0qbZu3aqioiJlZWXpb3/7W73nKCws1J///Gede+65+te//qWwsDCH4y1atJDVam3QeCsrK1VZ+fNfM6Wl9f/FgtNjsRgaM6RQn34Vpm++C3E4NvCaQ8oYtE9Ng23asz9Y987spJran7P0S2/GaNsX4aqo8lfPC0v04G3fqmlQrZatiT7TlwG4beyscCVcWK3+VzmvnuDswTSNc16vYyclJSk7O9v+dXZ2tnr37q3ExET7/uPHjysvL88eRpYtW6bOnTsrLi5OQ4YM0fz582XU8/9SQUGBrrjiCsXHx+udd96pE0RclZmZqcjISPsWGxvrVn9w9OBte3X+ucf1yJzz6hxbvbG50ifG687H41RYFKTHM75RYJOf/2Jc8K8Y/ffrcH31bVP989+t9X/vWJV2XdGZHD7gEW9vCFL2tkA9c99Rbw8FHmR4aPNVjSKMfPTRR6qpqdHRo0f1ySefKDExUb169VJOTo6kE+tDKisr7WFk3rx5GjJkiCSpb9++Kikp0bp16+r0fdttt6ljx456/fXXFRQUVO/5L7/8coWFhTlspzJhwgSVlJTYt8LCQjevHif9dei3uqLbEd2dGafvDwfWOV5+PECFB4OVXxCuCbPPV7uYCiV2P3zK/j7fFaroFtVqEkCJG2eX7G1NtOs7fzXv21KBvVopsFcrSdJND0fq6tHNvDw6wBxeX4Ldu3dvlZeXa8uWLTp8+LA6deqkli1bKjExUcOHD1dFRYVycnJ03nnnqW3btiooKNDmzZu1fPlySVJAQIAGDRqkefPm1Vn02q9fP61YsUJvvfWWbrrppnrPv3TpUnXp0qVBYw0KCjplqMHpMvTXoXuV2P2IMjLjdOCH3/7+WiySRVJgwKn/TujU9phKyvxVXeP1vA245KGhxzSy33GHfRcPPUdP33tU/3MF0zZnK6ZpnPN6GOnYsaPatGmj7OxsHT58WImJiZKkmJgYxcbGauPGjcrOztbVV18t6URVpKamRjExMfY+DMNQUFCQnn/+eUVG/nxr58MPP6yLLrpIt956qwzD0M0331zn/LGxserYsaPJV4lTeTB9r/pcVqxxz3ZUeYW/mkdWS5LKj/mrstpPMS0rldyzWHmfRejI0QC1alat2/7ngCqrLdr46Yn/r6/sdkTNI6v12c4wVVVb9KcLS5Xer0iL32G9CM68smMW7dz3823le/b7K/+rADWPsKmt1abiUov2Fvlr/w8ngnLB3hNtrS1sv9jq9hsbbVOHmJ8rfTv3+avsmEVFP/rpeKVF+V+d+HUe36FGgU1MvECcFsKIc14PI9KJqZqcnBwdPnxYDz74oH1/r1699O6772rz5s266667VFNTo0WLFumpp55Snz59HPoYMGCAXn31Vd15550O+ydOnCg/Pz+lpaXJMAwNGjTojFwTGmbgNd9Lkl58uMBh/7SX2+vfH56jqmqLusUd1eCUgwoPrVVxSYDyC8I1amoXHT564jduTa1FA5MP6b5bC2WxSPsOBum5JbH6V845Z/x6gK1fBuiae35+2N5fZ4dLkm679rgWPFKqtzcEaeT0n/9ounVylCRp0ogyTR5Z3uDz3PFEhNZ98vOUZvfhJxLMrje+V/vWTE/i7NJowkhGRoaqq6vtlRFJSkxM1OjRo1VVVaWkpCStWrVKhw8f1siRIx0qIJI0cOBAzZs3r04YkU5USPz9/ZWWliabzaZbbrnFfuzHH39UUZHjQseoqCgFBwf/uhuY4LLbejg9/sORQI19qpPTNpu2R2rT9kinbYAzpfel1ar96OApjw9LrdCw1IpTHq9Pff198Pyp10yh8eEJrM41mjBy/Phxde7cWdHRP5fWExMTdfToUfstwPPmzVNycnKdICKdCCMzZ87Uf//7X0VE1H2q4fjx4+Xn56ehQ4fKMAxdfvnlkqTk5OQ6bV999VUNHjzYg1cIAPg9Y5rGuUYRRtq3b1/vrbnt2rVz2L9y5cpT9vGnP/3JoW19/Y0bN07jxo1z2gYAAJxZjSKMAADgy6iMOEcYAQDAZKwZcY4wAgCAyaiMOMcToQAAgFdRGQEAwGSGPFAZ8chIGifCCAAAJmPNiHNM0wAAAK+iMgIAgMlYwOocYQQAAJMZhmQjjJwS0zQAAMCrqIwAAGAypmmcI4wAAGAy7qZxjmkaAADgVVRGAAAwmWFYZBgWt/vwVYQRAABMxpoR5wgjAACYjDUjzrFmBAAAH5SZmak//vGPCg8PV6tWrTRgwAAVFBQ4tKmoqFBGRoZatGihsLAwDRw4UAcPHnRos3fvXqWmpqpp06Zq1aqVHnzwQdXU1Hh0rIQRAABMZjM8s7li3bp1ysjI0KZNm7RmzRpVV1erT58+Ki8vt7e5//77tXLlSr3++utat26d9u/frxtvvNF+vLa2VqmpqaqqqtLGjRu1cOFCZWVladKkSZ761khimgYAANN5Y83I6tWrHb7OyspSq1attG3bNvXq1UslJSWaN2+elixZoquvvlqStGDBAnXp0kWbNm3SZZddpv/85z/64osv9P777ys6OlrdunXTtGnT9NBDD2nKlCkKDAx076J+QmUEAICzSGlpqcNWWVnZoNeVlJRIkpo3by5J2rZtm6qrq5WcnGxv07lzZ7Vt21a5ubmSpNzcXHXt2lXR0dH2NikpKSotLdXnn3/uqUsijAAAYDbDQ5skxcbGKjIy0r5lZmb+5vltNpvGjBmjK664QhdeeKEkqaioSIGBgYqKinJoGx0draKiInubXwaRk8dPHvMUpmkAADCZzZAsbk7TnFwzUlhYqIiICPv+oKCg33xtRkaGPvvsM3344YfuDcIkVEYAADiLREREOGy/FUZGjx6tVatWKTs7W23atLHvt1qtqqqq0pEjRxzaHzx4UFar1d7m13fXnPz6ZBtPIIwAAGCykwtY3d1cO6eh0aNHa/ny5frggw/UoUMHh+Pdu3dXkyZNtHbtWvu+goIC7d27VwkJCZKkhIQEbd++XYcOHbK3WbNmjSIiIhQfH3/635BfYZoGAACTnVjz4ebj4F1sn5GRoSVLluhf//qXwsPD7Ws8IiMjFRISosjISI0cOVJjx45V8+bNFRERoXvuuUcJCQm67LLLJEl9+vRRfHy8hg4dqpkzZ6qoqEiPPPKIMjIyGjQ91FCEEQAAfNCLL74oSerdu7fD/gULFmjYsGGSpGeeeUZ+fn4aOHCgKisrlZKSohdeeMHe1t/fX6tWrdJdd92lhIQEhYaGKj09XVOnTvXoWAkjAACYzJMLWBvKaMC8TnBwsObMmaM5c+acsk27du30zjvvuHZyFxFGAAAwGR+U5xxhBAAAkxmn8Tj3+vrwVdxNAwAAvIrKCAAAJvvlE1Td6cNXEUYAADCZzQNpxN1pnsaMaRoAAOBVVEYAADCZYVhkGG4+9MzN1zdmhBEAAExmayR9NFZM0wAAAK+iMgIAgMlYwOocYQQAAJMRRpxjmgYAAHgVlREAAExGZcQ5wggAACazySLJvVtzbW6+vjEjjAAAYDKb5H5lxBMDaaRYMwIAALyKyggAACYzDPfXfBisGQEAAKerVu6uGPHtT+1lmgYAAHgVlREAAExWa0gWpmlOiTACAIDJaggjTjFNAwAAvIrKCAAAJquVRRY3l7AaPPQMAACcLqZpnGOaBgAAeBWVEQAAzGZ4oLLhw5URwggAAKbzwMf2+nAaIYwAAGA2sohTrBkBAABeRWUEAADTURpxhjACAIDZDEMybO734aOYpgEAAF5FZQQAALMZHri314crI4QRAABMZ/tpc7cP38Q0DQAA8CoqIwAAmM2weWABq+9WRggjAACYjTDiFNM0AADAq6iMAABgOhawOkMYAQDAbEzTOEUYAQDAbDxnxCnWjAAAAK+iMgIAgOlYM+IMYQQAALOxZsQppmkAAIBXURkBAMBshuGByojvLmAljAAAYDrWjDjDNA0AAPAqKiMAAJiN54w4RRgBAMBs3E3jFNM0AADAq6iMAABgNiojThFGAAAwnfHT5m4fvokwAgCA6TxQGeHWXgAAcDaaM2eO2rdvr+DgYPXs2VObN2/29pDqIIwAAGC2k2tG3N1ctHTpUo0dO1aTJ0/Wxx9/rIsvvlgpKSk6dOiQCRd5+ggjAACY7eRzRtzdXPT0009r1KhRGj58uOLj4zV37lw1bdpU8+fPN+EiTx9rRtxg/PTGqK2p8vJIAPOUHq3x9hAA05SW1Ur6+fe5aWwe+Dn6qY/S0lKH3UFBQQoKCqrTvKqqStu2bdOECRPs+/z8/JScnKzc3Fz3x+NBhBE3HD16VJKUvz7LuwMBTNSsk7dHAJjv6NGjioyM9Hi/gYGBslqtKtq7xiP9hYWFKTY21mHf5MmTNWXKlDptf/jhB9XW1io6Otphf3R0tL788kuPjMdTCCNuiImJUWFhocLDw2WxWLw9HJ9XWlqq2NhYFRYWKiIiwtvDATyO9/iZZxiGjh49qpiYGFP6Dw4O1u7du1VV5ZkKumEYdf69qa8qcrYhjLjBz89Pbdq08fYwfnciIiL4RQ2fxnv8zDKjIvJLwcHBCg4ONvUc9TnnnHPk7++vgwcPOuw/ePCgrFbrGR+PMyxgBQDABwUGBqp79+5au3atfZ/NZtPatWuVkJDgxZHVRWUEAAAfNXbsWKWnp6tHjx7605/+pGeffVbl5eUaPny4t4fmgDCCs0ZQUJAmT57sE/OjQH14j8PTBg0apO+//16TJk1SUVGRunXrptWrV9dZ1OptFsP0+5kAAABOjTUjAADAqwgjAADAqwgjAADAqwgjAADAqwgjMN2wYcNksVj0xBNPOOxfsWKF/UmCOTk5slgs9q1ly5a67rrrtH379nr7+vXWt29fe5v27dvLYrHotddeqzOWCy64QBaLRVlZWZ6/UPxuzZ07V+Hh4aqp+fnzR8rKytSkSRP17t3boe3J9/quXbskSbm5ufL391dqamqdfvfs2SOLxaL8/Hz7vqNHjyopKUnx8fHat2+fvU1926ZNm0y5XsDTCCM4I4KDgzVjxgwdPnzYabuCggIdOHBA7733niorK5WamlrnMcp9+/bVgQMHHLZXX33VoU1sbKwWLFjgsG/Tpk0qKipSaGioZy4K+ElSUpLKysq0detW+74NGzbIarUqLy9PFRUV9v3Z2dlq27atzj//fEnSvHnzdM8992j9+vXav3+/0/N8//33SkpKUnl5uTZs2ODwBOj333+/zs9F9+7dPXylgDkIIzgjkpOTZbValZmZ6bRdq1atZLVademll2rMmDEqLCys84FOQUFBslqtDluzZs0c2qSlpWndunUqLCy075s/f77S0tIUEMDjdeBZcXFxat26tXJycuz7cnJy1L9/f3Xo0MGhQpGTk6OkpCRJJ6onS5cu1V133aXU1FSnFbvCwkJdddVVioyM1AcffKAWLVo4HG/RokWdn4smTZp49DoBsxBGcEb4+/tr+vTpmj17tvbt2/eb7UtKSuzTLIGBgS6fLzo6WikpKVq4cKEk6dixY1q6dKlGjBjhcl9AQyQlJSk7O9v+dXZ2tnr37q3ExET7/uPHjysvL88eRpYtW6bOnTsrLi5OQ4YM0fz58+v9KPuCggJdccUVio+P1zvvvKOwsLAzc1HAGUIYwRlzww03qFu3bpo8efIp27Rp00ZhYWGKiorSkiVL1K9fP3Xu3NmhzapVqxQWFuawTZ8+vU5fI0aMUFZWlgzD0BtvvKHzzz9f3bp18/RlAZJOhJGPPvpINTU1Onr0qD755BMlJiaqV69e9opJbm6uKisr7WFk3rx5GjJkiKQT048lJSVat25dnb5vu+02dezYUa+//vopn856+eWX1/m5AM4W1KtxRs2YMUNXX321HnjggXqPb9iwQU2bNtWmTZs0ffp0zZ07t06bpKQkvfjiiw77mjdvXqddamqq/vKXv2j9+vWaP38+VRGYqnfv3iovL9eWLVt0+PBhderUSS1btlRiYqKGDx+uiooK5eTk6LzzzlPbtm1VUFCgzZs3a/ny5ZKkgIAADRo0SPPmzauz6LVfv35asWKF3nrrLd100031nn/p0qXq0qWL2ZcJmIIwgjOqV69eSklJ0YQJEzRs2LA6xzt06KCoqCjFxcXp0KFDGjRokNavX+/QJjQ0VB07dvzNcwUEBGjo0KGaPHmy8vLy7L/0ATN07NhRbdq0UXZ2tg4fPqzExERJUkxMjGJjY7Vx40ZlZ2fr6quvlnSiKlJTU6OYmBh7H4ZhKCgoSM8//7zDx9o//PDDuuiii3TrrbfKMAzdfPPNdc4fGxvboJ8LoDFimgZn3BNPPKGVK1cqNzfXabuMjAx99tlnboWIESNGaN26derfv3+dRa6ApyUlJSknJ0c5OTkO1Y1evXrp3Xff1ebNm5WUlKSamhotWrRITz31lPLz8+3bp59+qpiYmDp3h0nSxIkTNWXKFKWlpWnp0qVn8KoA81EZwRnXtWtXpaWladasWU7bNW3aVKNGjdLkyZM1YMAA+zNJKisrVVRU5NA2ICBA55xzTp0+unTpoh9++EFNmzb13AUAp5CUlKSMjAxVV1fbKyOSlJiYqNGjR6uqqkpJSUlatWqVDh8+rJEjRzpUQCRp4MCBmjdvnu688846/T/88MPy9/dXWlqabDabbrnlFvuxH3/8sc7PRVRUlIKDgz18lYDnURmBV0ydOlU2m+03240ePVo7duzQ66+/bt+3evVqtW7d2mG78sorT9lHixYtFBIS4pFxA84kJSXp+PHj6tixo8NHtCcmJuro0aP2W4DnzZun5OTkOkFEOhFGtm7dqv/+97/1nmP8+PGaPn26hg4dqiVLltj3Jycn1/m5WLFihcevETCDxajvPjIAAIAzhMoIAADwKsIIAADwKsIIAADwKsIIAADwKsIIAADwKsIIAADwKsIIAADwKsIIAADwKsIIcJYbNmyYBgwYYP+6d+/eGjNmzBkfR05OjiwWi44cOXLKNhaLxaWngk6ZMkXdunVza1x79uyRxWJRfn6+W/0AMA9hBDDBsGHDZLFYZLFYFBgYqI4dO2rq1Kmqqakx/dxvvfWWpk2b1qC2DQkQAGA2PigPMEnfvn21YMECVVZW6p133lFGRoaaNGmiCRMm1GlbVVWlwMBAj5y3efPmHukHAM4UKiOASYKCgmS1WtWuXTvdddddSk5O1ttvvy3p56mVxx9/XDExMYqLi5MkFRYW6uabb1ZUVJSaN2+u/v37a8+ePfY+a2trNXbsWEVFRalFixYaN26cfv3xUr+epqmsrNRDDz2k2NhYBQUFqWPHjpo3b5727NmjpKQkSVKzZs1ksVg0bNgwSZLNZlNmZqY6dOigkJAQXXzxxXrjjTcczvPOO++oU6dOCgkJUVJSksM4G+qhhx5Sp06d1LRpU5133nmaOHGiqqur67R76aWXFBsbq6ZNm+rmm29WSUmJw/F//OMf6tKli4KDg9W5c2e98MILLo8FgPcQRoAzJCQkRFVVVfav165dq4KCAq1Zs0arVq1SdXW1UlJSFB4erg0bNuijjz5SWFiY+vbta3/dU089paysLM2fP18ffvihiouLtXz5cqfnve222/Tqq69q1qxZ2rFjh1566SWFhYUpNjZWb775piSpoKBABw4c0HPPPSdJyszM1KJFizR37lx9/vnnuv/++zVkyBCtW7dO0onQdOONN+r6669Xfn6+br/9do0fP97l70l4eLiysrL0xRdf6LnnntMrr7yiZ555xqHNzp07tWzZMq1cuVKrV6/WJ598orvvvtt+fPHixZo0aZIef/xx7dixQ9OnT9fEiRO1cOFCl8cDwEsMAB6Xnp5u9O/f3zAMw7DZbMaaNWuMoKAg44EHHrAfj46ONiorK+2v+ec//2nExcUZNpvNvq+ystIICQkx3nvvPcMwDKN169bGzJkz7cerq6uNNm3a2M9lGIaRmJho3HfffYZhGEZBQYEhyVizZk2948zOzjYkGYcPH7bvq6ioMJo2bWps3LjRoe3IkSONW265xTAMw5gwYYIRHx/vcPyhhx6q09evSTKWL19+yuN///vfje7du9u/njx5suHv72/s27fPvu/dd981/Pz8jAMHDhiGYRjnn3++sWTJEod+pk2bZiQkJBiGYRi7d+82JBmffPLJKc8LwLtYMwKYZNWqVQoLC1N1dbVsNptuvfVWTZkyxX68a9euDutEPv30U+3cuVPh4eEO/VRUVGjXrl0qKSnRgQMH1LNnT/uxgIAA9ejRo85UzUn5+fny9/dXYmJig8e9c+dOHTt2TH/+858d9ldVVemSSy6RJO3YscNhHJKUkJDQ4HOctHTpUs2aNUu7du1SWVmZampqFBER4dCmbdu2Ovfccx3OY7PZVFBQoPDwcO3atUsjR47UqFGj7G1qamoUGRnp8ngAeAdhBDBJUlKSXnzxRQUGBiomJkYBAY4/bqGhoQ5fl5WVqXv37lq8eHGdvlq2bHlaYwgJCXH5NWVlZZKkf//73w4hQDqxDsZTcnNzlZaWpkcffVQpKSmKjIzUa6+9pqeeesrlsb7yyit1wpG/v7/HxgrAXIQRwCShoaHq2LFjg9tfeumlWrp0qVq1alWnOnBS69atlZeXp169ekk6UQHYtm2bLr300nrbd+3aVTabTevWrVNycnKd4ycrM7W1tfZ98fHxCgoK0t69e09ZUenSpYt9Me5JmzZt+u2L/IWNGzeqXbt2evjhh+37vv322zrt9u7dq/379ysmJsZ+Hj8/P8XFxSk6OloxMTH65ptvlJaW5tL5ATQeLGAFGom0tDSdc8456t+/vzZs2KDdu3crJydH9957r/bt2ydJuu+++/TEE09oxYoV+vLLL3X33Xc7fUZI+/btlZ6erhEjRmjFihX2PpctWyZJateunSwWi1atWqXvv/9eZWVlCg8P1wMPPKD7779fCxcu1K5du/Txxx9r9uzZ9kWhd955p77++ms9+OCDKigo0JIlS5SVleXS9f7hD3/Q3r179dprr2nXrl2aNWtWvYtxg4ODlZ6erk8//VQbNmzQvffeq5tvvllWq1WS9OijjyozM1OzZs3SV199pe3bt2vBggV6+umnXRoPAO8hjACNRNOmTbV+/Xq1bdtWN954o7p06aKRI0eqoqLCXin561//qqFDhyo9PV0JCQkKDw/XDTfc4LTfF198Uf/7v/+ru+++W507d9aoUaNUXl4uSTr33HP16KOPavz48YqOjtbo0aMlSdOmTdPEiROVmZmpLl26qG/fvvr3v/+tDh06SDqxjuPNN9/UihUrdPHFF2vu3LmaPn26S9fbr18/3X///Ro9erS6deumjRs3auLEiXXadezYUTfeeKOuu+469enTRxdddJHDrbu33367/vGPf2jBggXq2rWrEhMTlZWVZR8rgMbPYpxq5RsAAMAZQGUEAAB4FWEEAAB4FWEEAAB4FWEEAAB4FWEEAAB4FWEEAAB4FWEEAAB4FWEEAAB4FWEEAAB4FWEEAAB4FWEEAAB41f8DpJX1WVLmI6YAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ConfusionMatrixDisplay.from_predictions(labels, preds, display_labels=['NREM', 'WAKE'], cmap='cividis')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "5TyAmKh3ch3W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "b8b15191-5436-49f8-f4fc-cbb12af59f79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional Neural Networks (CNNs), in the context of Local Field Potential (LFP) signals from rats, function by extracting meaningful patterns from the raw signal data. Just as they analyze pixel patterns in images, CNNs can interpret temporal sequences in LFP data, detecting features that differentiate between various brain states like 'NREM' sleep and wakefulness. In the code we refined, CNNs utilize 1D convolutional layers to slide across the temporal dimension of the signals, capturing important features like signal oscillations and frequency components that are indicative of different sleep stages. The architecture is designed to learn from these biological signals, with convolutional layers detecting features, pooling layers reducing dimensionality to the most salient features, and fully connected layers making final predictions on the rats' sleep states. By training this network on labeled LFP data, it learns to classify segments of LFP recordings into 'NREM' or 'WAKE' states, offering insights into the brain activity patterns that characterize these states."
      ],
      "metadata": {
        "id": "tNmJWS7-fKn7"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "87560d5e995046ffb44774fa17ba9e25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_54f2646779064834be9b935222e5a223",
              "IPY_MODEL_2626cfd645d4431a82d3ac1ef8ccf754",
              "IPY_MODEL_8cb456ad37cf4dfa8c0f53a4311cc987"
            ],
            "layout": "IPY_MODEL_bd8b97d392f749338e68afa5e09914e7"
          }
        },
        "54f2646779064834be9b935222e5a223": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdf15c8e125a4272b805eccea916814c",
            "placeholder": "â",
            "style": "IPY_MODEL_b7e1a2fec5484bd8843c89264ea8ab47",
            "value": "100%"
          }
        },
        "2626cfd645d4431a82d3ac1ef8ccf754": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a97e9aed3c64e0f90dea9b04ac84e74",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_64059bf2d5ab4d7d991a4dd6a1357c45",
            "value": 10
          }
        },
        "8cb456ad37cf4dfa8c0f53a4311cc987": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7331746bd40e4005a4fce730d90c90f1",
            "placeholder": "â",
            "style": "IPY_MODEL_25537d1851e846e49cf8b80a4418e90d",
            "value": "â10/10â[07:14&lt;00:00,â43.50s/it]"
          }
        },
        "bd8b97d392f749338e68afa5e09914e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdf15c8e125a4272b805eccea916814c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7e1a2fec5484bd8843c89264ea8ab47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a97e9aed3c64e0f90dea9b04ac84e74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64059bf2d5ab4d7d991a4dd6a1357c45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7331746bd40e4005a4fce730d90c90f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25537d1851e846e49cf8b80a4418e90d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}